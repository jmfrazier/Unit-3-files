{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.1 Credit Card Fraud\n",
    "I am going to use a data set to try and predict credit card fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the data\n",
    "data = pd.read_csv(r'C:\\Users\\jmfra\\OneDrive\\Documents\\Thinkful Data Science Files\\3.6.1 data\\creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets take a look \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ff2cfe9908>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAF0CAYAAACDhlvVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xu0XnV95/H3BxCoVILIeKIjtDq0EKwoOVyXFdF0ES+0\nWu0qHmQpomOtiKw4UlcdrRnodBxaDUtu4wgUL3BmMVAvFSQIbUGFkkpoBQnYWjRaTfQUPKFBCJDv\n/LH3sQ/P5HbCye/E5P1a61lZz/599rP3DiTnk9++PKkqJEmSWtlltndAkiTtXCwfkiSpKcuHJElq\nyvIhSZKasnxIkqSmLB+SJKkpy4ckSWrK8iFJkpqyfEiSpKYsH5IkqalplY8k70jyD0km+9ctSV4x\nlDkryQ+SPJTky0kOHBrfI8kFSSaSPJjkqiTPHMo8Pcnl/TYeSHJxkr2GMvsnuSbJ2iSrkpyTZJeh\nzKFJbk7y0yTfTXLmdI5XkiTNvOnOfHwPeB8wHxgF/gr4fJJ5AEneB7wLeDtwJLAWWJpk94HPOBd4\nNfB64Fjg2cDVQ9u5ApgHLOizxwIfnxrsS8a1wG7A0cCbgVOAswYyTwOWAvf1+3smsDjJ26Z5zJIk\naQblyX6xXJJ/Bd5bVX+e5AfAn1bVkn5sb2A18OaqurJ//2PgDVX12T5zELACOLqqlvVF5pvAaFXd\n0WcWAtcAz6mqVUleCXwBeFZVTfSZ3wM+DPyHqnosye8DZwNzq+qxPvM/gNdU1SFP6qAlSdJW2+pr\nPpLskuQNwFOBW5I8F5gL3DiVqao1wG3AMf2iw+lmKwYz9wIrBzJHAw9MFY/eDUABRw1k7pwqHr2l\nwBzg+QOZm6eKx0DmoCRztuqgJUnSkzbt8pHk15I8CDwCXAj8dl8g5tIVhNVDq6zuxwBGgHV9KdlY\nZi7wo8HBqnocuH8os6HtMM2MJElqbLetWOce4IV0swy/A3wqybEzulezKMkzgIXAd4CHZ3dvJEn6\nubIn8MvA0qr6142Fpl0++tMY/9y/vSPJkcAZwDlA6GY3BmccRoCpUyirgN2T7D00+zHSj01lhu9+\n2RXYdyhzxNCujQyMTf06spnMhiwELt/EuCRJ2rQ30t08skFbM/MxbBdgj6q6L8kqujtUvgE/u+D0\nKOCCPns78FifGbzg9ADg1j5zK7BPksMGrvtYQFdsbhvIvD/JfgPXfRwPTAJ3D2T+OMmu/Wmbqcy9\nVTW5ieP5DsBnPvMZ5s2bN63fCG2fFi1axJIlS2Z7NyRthH9GdxwrVqzg5JNPhv5n6cZMq3wk+RPg\nS3QXiD6Nrtm8lO6HOnS30X4gyT/1Gz4b+D7weeguQE1yCfDRJA8ADwIfA75WVcv6zD1JlgKf6O9Y\n2R04DxivqqkZi+vpSsan+9t7n9Vv6/yqerTPXAH8EXBpkv8JvAB4N90szaY8DDBv3jzmz58/nd8e\nbafmzJnjf0tpO+af0R3SJi9bmO7MxzOBT9L9sJ+km+E4vqr+CqCqzknyVLpncuwDfAV4ZVWtG/iM\nRcDjwFXAHsB1wGlD2zkJOJ/uLpf1ffZnpaGq1ic5AbgIuIXueSKXAR8ayKxJcjzdrMvXgQlgcVVd\nMs1jliRJM2ha5aOqNvuArqpaDCzexPgjwOn9a2OZnwAnb2Y73wNO2EzmLrqZGUmStJ3wu10kSVJT\nlg/t8MbGxmZ7FyRtgn9Gdz6WD+3w/ItN2r75Z3TnY/mQJElNWT4kSVJTlg9JktSU5UOSJDVl+ZAk\nSU1ZPiRJUlOWD0mS1NRMfKutfk6tXLmSiYmJzQf1c2G//fbjgAMOmO3dkKTNsnzspFauXMlBB83j\n4Ycfmu1d0QzZc8+ncu+9KywgkrZ7lo+d1MTERF88PgPMm+3d0ZO2gocfPpmJiQnLh6TtnuVjpzcP\nmD/bOyFJ2ol4wakkSWrK8iFJkpqyfEiSpKYsH5IkqSnLhyRJasryIUmSmrJ8SJKkpiwfkiSpKcuH\nJElqyvIhSZKasnxIkqSmLB+SJKkpy4ckSWrK8iFJkpqyfEiSpKYsH5IkqSnLhyRJasryIUmSmrJ8\nSJKkpiwfkiSpKcuHJElqyvIhSZKasnxIkqSmLB+SJKkpy4ckSWrK8iFJkpqyfEiSpKamVT6S/GGS\nZUnWJFmd5LNJfnUo8+dJ1g+9rh3K7JHkgiQTSR5MclWSZw5lnp7k8iSTSR5IcnGSvYYy+ye5Jsna\nJKuSnJNkl6HMoUluTvLTJN9NcuZ0jlmSJM2s6c58vAQ4DzgK+A3gKcD1SX5hKPclYASY27/GhsbP\nBV4NvB44Fng2cPVQ5gpgHrCgzx4LfHxqsC8Z1wK7AUcDbwZOAc4ayDwNWArcB8wHzgQWJ3nbNI9b\nkiTNkN2mE66qVw2+T3IK8CNgFPjqwNAjVfXjDX1Gkr2BU4E3VNVN/bK3ACuSHFlVy5LMAxYCo1V1\nR585HbgmyXuralU/fjDwsqqaAO5M8kHgw0kWV9VjwMl0Bemt/fsVSQ4D3gNcPJ1jlyRJM+PJXvOx\nD1DA/UPLj+tPy9yT5MIk+w6MjdKVnhunFlTVvcBK4Jh+0dHAA1PFo3dDv62jBjJ39sVjylJgDvD8\ngczNffEYzByUZM70DlWSJM2ErS4fSUJ3+uSrVXX3wNCXgDcBLwf+AHgpcG2fh+40zLqqWjP0kav7\nsanMjwYHq+pxupIzmFm9gc9gmhlJktTQtE67DLkQOAR48eDCqrpy4O03k9wJfBs4DvjrJ7G9phYt\nWsScOU+cHBkbG2NsbPjyFUmSdj7j4+OMj48/Ydnk5OQWrbtV5SPJ+cCrgJdU1Q83la2q+5JMAAfS\nlY9VwO5J9h6a/Rjpx+h/Hb77ZVdg36HMEUObGxkYm/p1ZDOZDVqyZAnz58/fVESSpJ3Whv5Bvnz5\nckZHRze77rRPu/TF4zV0F3qu3IL8c4BnAFMl5XbgMbq7WKYyBwEHALf2i24F9ukvDp2yAAhw20Dm\nBUn2G8gcD0wCdw9kju2Ly2Dm3qrasnomSZJm1HSf83Eh8EbgJGBtkpH+tWc/vlf/rI2jkvxSkgXA\n54Bv0V3oST/bcQnw0STHJRkFLgW+VlXL+sw9ff4TSY5I8mK6W3zH+ztdAK6nKxmf7p/lsRA4Gzi/\nqh7tM1cA64BLkxyS5ETg3cBHpv9bJUmSZsJ0T7u8g+6Ok78ZWv4W4FPA48ChdBec7gP8gK5E/NFA\nIQBY1GevAvYArgNOG/rMk4Dz6e5yWd9nz5garKr1SU4ALgJuAdYClwEfGsisSXI8cAHwdWACWFxV\nl0zzuCVJ0gyZ7nM+NjlTUlUPA6/Ygs95BDi9f20s8xO653Rs6nO+B5ywmcxddHfcSJKk7YDf7SJJ\nkpqyfEiSpKYsH5IkqSnLhyRJasryIUmSmrJ8SJKkpiwfkiSpKcuHJElqyvIhSZKasnxIkqSmLB+S\nJKkpy4ckSWrK8iFJkpqyfEiSpKYsH5IkqSnLhyRJasryIUmSmrJ8SJKkpiwfkiSpKcuHJElqyvIh\nSZKasnxIkqSmLB+SJKkpy4ckSWrK8iFJkpqyfEiSpKYsH5IkqSnLhyRJasryIUmSmrJ8SJKkpiwf\nkiSpKcuHJElqyvIhSZKasnxIkqSmLB+SJKkpy4ckSWrK8iFJkpqyfEiSpKYsH5IkqSnLhyRJampa\n5SPJHyZZlmRNktVJPpvkVzeQOyvJD5I8lOTLSQ4cGt8jyQVJJpI8mOSqJM8cyjw9yeVJJpM8kOTi\nJHsNZfZPck2StUlWJTknyS5DmUOT3Jzkp0m+m+TM6RyzJEmaWdOd+XgJcB5wFPAbwFOA65P8wlQg\nyfuAdwFvB44E1gJLk+w+8DnnAq8GXg8cCzwbuHpoW1cA84AFffZY4OMD29kFuBbYDTgaeDNwCnDW\nQOZpwFLgPmA+cCawOMnbpnnckiRphuw2nXBVvWrwfZJTgB8Bo8BX+8VnAGdX1Rf7zJuA1cBrgSuT\n7A2cCryhqm7qM28BViQ5sqqWJZkHLARGq+qOPnM6cE2S91bVqn78YOBlVTUB3Jnkg8CHkyyuqseA\nk+kK0lv79yuSHAa8B7h4OscuSZJmxpO95mMfoID7AZI8F5gL3DgVqKo1wG3AMf2iw+lKz2DmXmDl\nQOZo4IGp4tG7od/WUQOZO/viMWUpMAd4/kDm5r54DGYOSjJnK45XkiQ9SVtdPpKE7vTJV6vq7n7x\nXLqCsHoovrofAxgB1vWlZGOZuXQzKj9TVY/TlZzBzIa2wzQzkiSpoWmddhlyIXAI8OIZ2hdJkrQT\n2KrykeR84FXAS6rqhwNDq4DQzW4MzjiMAHcMZHZPsvfQ7MdIPzaVGb77ZVdg36HMEUO7NjIwNvXr\nyGYyG7Ro0SLmzHnimZmxsTHGxsY2tZokSTuF8fFxxsfHn7BscnJyi9addvnoi8drgJdW1crBsaq6\nL8kqujtUvtHn96a7TuOCPnY78Fif+WyfOQg4ALi1z9wK7JPksIHrPhbQFZvbBjLvT7LfwHUfxwOT\nwN0DmT9Osmt/2mYqc29VbfJ3aMmSJcyfP39LfkskSdrpbOgf5MuXL2d0dHSz6073OR8XAm8ETgLW\nJhnpX3sOxM4FPpDkN5O8APgU8H3g8/CzC1AvAT6a5Lgko8ClwNeqalmfuYfuwtBPJDkiyYvpbvEd\n7+90AbiermR8un+Wx0LgbOD8qnq0z1wBrAMuTXJIkhOBdwMfmc5xS5KkmTPdmY930F1Q+jdDy99C\nVzKoqnOSPJXumRz7AF8BXllV6wbyi4DHgauAPYDrgNOGPvMk4Hy6u1zW99kzpgaran2SE4CLgFvo\nnidyGfChgcyaJMfTzbp8HZgAFlfVJdM8bkmSNEOm+5yPLZopqarFwOJNjD8CnN6/Npb5Cd1zOja1\nne8BJ2wmcxfw0k1lJElSO363iyRJasryIUmSmrJ8SJKkpiwfkiSpKcuHJElqyvIhSZKasnxIkqSm\nLB+SJKkpy4ckSWrK8iFJkpqyfEiSpKYsH5IkqSnLhyRJasryIUmSmrJ8SJKkpiwfkiSpKcuHJElq\nyvIhSZKasnxIkqSmLB+SJKkpy4ckSWrK8iFJkpqyfEiSpKYsH5IkqSnLhyRJasryIUmSmrJ8SJKk\npiwfkiSpKcuHJElqyvIhSZKasnxIkqSmLB+SJKkpy4ckSWrK8iFJkpqyfEiSpKYsH5IkqSnLhyRJ\nasryIUmSmrJ8SJKkpiwfkiSpKcuHJElqatrlI8lLknwhyb8kWZ/kt4bG/7xfPvi6diizR5ILkkwk\neTDJVUmeOZR5epLLk0wmeSDJxUn2Gsrsn+SaJGuTrEpyTpJdhjKHJrk5yU+TfDfJmdM9ZkmSNHO2\nZuZjL+DvgXcCtZHMl4ARYG7/GhsaPxd4NfB64Fjg2cDVQ5krgHnAgj57LPDxqcG+ZFwL7AYcDbwZ\nOAU4ayDzNGApcB8wHzgTWJzkbVt+uJIkaSbtNt0Vquo64DqAJNlI7JGq+vGGBpLsDZwKvKGqbuqX\nvQVYkeTIqlqWZB6wEBitqjv6zOnANUneW1Wr+vGDgZdV1QRwZ5IPAh9OsriqHgNOBp4CvLV/vyLJ\nYcB7gIune+ySJOnJ21bXfByXZHWSe5JcmGTfgbFRutJz49SCqroXWAkc0y86Gnhgqnj0bqCbaTlq\nIHNnXzymLAXmAM8fyNzcF4/BzEFJ5jypI5QkSVtlW5SPLwFvAl4O/AHwUuDagVmSucC6qloztN7q\nfmwq86PBwap6HLh/KLN6A5/BNDOSJKmhaZ922ZyqunLg7TeT3Al8GzgO+OuZ3p4kSfr5MuPlY1hV\n3ZdkAjiQrnysAnZPsvfQ7MdIP0b/6/DdL7sC+w5ljhja3MjA2NSvI5vJbNCiRYuYM+eJZ2bGxsYY\nGxu+dlaSpJ3P+Pg44+PjT1g2OTm5Retu8/KR5DnAM4Af9otuBx6ju4vls33mIOAA4NY+cyuwT5LD\nBq77WAAEuG0g8/4k+w1c93E8MAncPZD54yS79qdtpjL3VtUmf4eWLFnC/Pnzt+aQJUna4W3oH+TL\nly9ndHR0s+tuzXM+9krywiQv6hc9r3+/fz92TpKjkvxSkgXA54Bv0V3oST/bcQnw0STHJRkFLgW+\nVlXL+sw9ff4TSY5I8mLgPGC8v9MF4Hq6kvHp/lkeC4GzgfOr6tE+cwWwDrg0ySFJTgTeDXxkusct\nSZJmxtbMfBxOd/qk+tfUD/JP0j3741C6C073AX5AVyL+aKAQACwCHgeuAvagu3X3tKHtnAScT3eX\ny/o+e8bUYFWtT3ICcBFwC7AWuAz40EBmTZLjgQuArwMTwOKqumQrjluSJM2ArXnOx01sesbkFVvw\nGY8Ap/evjWV+Qvecjk19zveAEzaTuYvujhtJkrQd8LtdJElSU5YPSZLUlOVDkiQ1ZfmQJElNWT4k\nSVJTlg9JktSU5UOSJDVl+ZAkSU1ZPiRJUlOWD0mS1JTlQ5IkNWX5kCRJTVk+JElSU5YPSZLUlOVD\nkiQ1ZfmQJElNWT4kSVJTlg9JktSU5UOSJDVl+ZAkSU1ZPiRJUlOWD0mS1JTlQ5IkNWX5kCRJTVk+\nJElSU5YPSZLUlOVDkiQ1ZfmQJElNWT4kSVJTlg9JktSU5UOSJDVl+ZAkSU1ZPiRJUlOWD0mS1JTl\nQ5IkNWX5kCRJTVk+JElSU5YPSZLUlOVDkiQ1ZfmQJElNWT4kSVJT0y4fSV6S5AtJ/iXJ+iS/tYHM\nWUl+kOShJF9OcuDQ+B5JLkgykeTBJFcleeZQ5ulJLk8ymeSBJBcn2Wsos3+Sa5KsTbIqyTlJdhnK\nHJrk5iQ/TfLdJGdO95glSdLM2ZqZj72AvwfeCdTwYJL3Ae8C3g4cCawFlibZfSB2LvBq4PXAscCz\ngauHPuoKYB6woM8eC3x8YDu7ANcCuwFHA28GTgHOGsg8DVgK3AfMB84EFid521YctyRJmgG7TXeF\nqroOuA4gSTYQOQM4u6q+2GfeBKwGXgtcmWRv4FTgDVV1U595C7AiyZFVtSzJPGAhMFpVd/SZ04Fr\nkry3qlb14wcDL6uqCeDOJB8EPpxkcVU9BpwMPAV4a/9+RZLDgPcAF0/32CVJ0pM3o9d8JHkuMBe4\ncWpZVa0BbgOO6RcdTld6BjP3AisHMkcDD0wVj94NdDMtRw1k7uyLx5SlwBzg+QOZm/viMZg5KMmc\nrTxMSZL0JMz0Badz6QrC6qHlq/sxgBFgXV9KNpaZC/xocLCqHgfuH8psaDtMMyNJkhqa9mmXncWi\nRYuYM+eJkyNjY2OMjY3N0h5JkrT9GB8fZ3x8/AnLJicnt2jdmS4fq4DQzW4MzjiMAHcMZHZPsvfQ\n7MdIPzaVGb77ZVdg36HMEUPbHxkYm/p1ZDOZDVqyZAnz58/fVESSpJ3Whv5Bvnz5ckZHRze77oye\ndqmq++h+qC+YWtZfYHoUcEu/6HbgsaHMQcABwK39oluBffqLQ6csoCs2tw1kXpBkv4HM8cAkcPdA\n5ti+uAxm7q2qLatnkiRpRm3Ncz72SvLCJC/qFz2vf79///5c4ANJfjPJC4BPAd8HPg8/uwD1EuCj\nSY5LMgpcCnytqpb1mXvoLgz9RJIjkrwYOA8Y7+90AbiermR8un+Wx0LgbOD8qnq0z1wBrAMuTXJI\nkhOBdwMfme5xS5KkmbE1p10OB/6a7sLS4t9/kH8SOLWqzknyVLpncuwDfAV4ZVWtG/iMRcDjwFXA\nHnS37p42tJ2TgPPp7nJZ32fPmBqsqvVJTgAuoptVWQtcBnxoILMmyfHABcDXgQlgcVVdshXHLUmS\nZsDWPOfjJjYzY1JVi4HFmxh/BDi9f20s8xO653RsajvfA07YTOYu4KWbykiSpHb8bhdJktSU5UOS\nJDVl+ZAkSU1ZPiRJUlOWD0mS1JTlQ5IkNWX5kCRJTVk+JElSU5YPSZLUlOVDkiQ1ZfmQJElNWT4k\nSVJTlg9JktSU5UOSJDVl+ZAkSU1ZPiRJUlOWD0mS1JTlQ5IkNWX5kCRJTVk+JElSU5YPSZLUlOVD\nkiQ1ZfmQJElNWT4kSVJTlg9JktSU5UOSJDVl+ZAkSU1ZPiRJUlOWD0mS1JTlQ5IkNWX5kCRJTVk+\nJElSU5YPSZLUlOVDkiQ1ZfmQJElNWT4kSVJTlg9JktSU5UOSJDVl+ZAkSU1ZPiRJUlMzXj6SfCjJ\n+qHX3UOZs5L8IMlDSb6c5MCh8T2SXJBkIsmDSa5K8syhzNOTXJ5kMskDSS5OstdQZv8k1yRZm2RV\nknOSWLgkSZpF2+oH8V3ACDC3f/361ECS9wHvAt4OHAmsBZYm2X1g/XOBVwOvB44Fng1cPbSNK4B5\nwII+eyzw8YHt7AJcC+wGHA28GTgFOGtmDlGSJG2N3bbR5z5WVT/eyNgZwNlV9UWAJG8CVgOvBa5M\nsjdwKvCGqrqpz7wFWJHkyKpalmQesBAYrao7+szpwDVJ3ltVq/rxg4GXVdUEcGeSDwIfTrK4qh7b\nRscuSZI2YVvNfPxKkn9J8u0kn0myP0CS59LNhNw4FayqNcBtwDH9osPpStFg5l5g5UDmaOCBqeLR\nuwEo4KiBzJ198ZiyFJgDPH9GjlKSJE3btigff0t3emMh8A7gucDN/fUYc+kKwuqhdVb3Y9CdrlnX\nl5KNZeYCPxocrKrHgfuHMhvaDgMZSZLU2IyfdqmqpQNv70qyDPgu8LvAPTO9PUmS9PNlW13z8TNV\nNZnkW8CBwN8AoZvdGJyVGAGmTqGsAnZPsvfQ7MdIPzaVGb77ZVdg36HMEUO7MzIwtkmLFi1izpw5\nT1g2NjbG2NjY5laVJGmHNz4+zvj4+BOWTU5ObtG627x8JPlFuuLxyaq6L8kqujtUvtGP7013ncYF\n/Sq3A4/1mc/2mYOAA4Bb+8ytwD5JDhu47mMBXbG5bSDz/iT7DVz3cTwwCTzh1t8NWbJkCfPnz9+6\ng5YkaQe3oX+QL1++nNHR0c2uO+PlI8mfAn9Jd6rlPwL/DXgU+D995FzgA0n+CfgOcDbwfeDz0F2A\nmuQS4KNJHgAeBD4GfK2qlvWZe5IsBT6R5PeB3YHzgPH+TheA6+lKxqf723uf1W/r/Kp6dKaPW5Ik\nbZltMfPxHLpncDwD+DHwVeDoqvpXgKo6J8lT6Z7JsQ/wFeCVVbVu4DMWAY8DVwF7ANcBpw1t5yTg\nfLq7XNb32TOmBqtqfZITgIuAW+ieJ3IZ8KEZPFZJkjRN2+KC081eFFFVi4HFmxh/BDi9f20s8xPg\n5M1s53vACZvbH0mS1I6PGpckSU1ZPiRJUlOWD0mS1JTlQ5IkNWX5kCRJTVk+JElSU5YPSZLUlOVD\nkiQ1ZfmQJElNWT4kSVJTlg9JktSU5UOSJDVl+ZAkSU1ZPiRJUlOWD0mS1JTlQ5IkNWX5kCRJTVk+\nJElSU5YPSZLUlOVDkiQ1ZfmQJElNWT4kSVJTlg9JktSU5UOSJDVl+ZAkSU1ZPiRJUlOWD0mS1JTl\nQ5IkNWX5kCRJTVk+JElSU5YPSZLUlOVDkiQ1ZfmQJElNWT4kSVJTlg9JktSU5UOSJDVl+ZAkSU1Z\nPiRJUlOWD0mS1JTlQ5IkNWX5kCRJTe0U5SPJaUnuS/LTJH+b5IjZ3ie1ND7bOyBpE8bH/TO6s9nh\ny0eSE4GPAB8CDgP+AViaZL9Z3TE15F9s0vbM8rHz2eHLB7AI+HhVfaqq7gHeATwEnDq7uyVJ0s5p\nhy4fSZ4CjAI3Ti2rqgJuAI6Zrf2SJGlnttts78A2th+wK7B6aPlq4KD2uyNJW2blypVMTEzM9m40\nMTk5yfLly2d7N7a5/fbbjwMOOGC2d2O7sKOXj62xJ8CKFStmez+2qX8/vmuBHftY4fvA5bO9E9vY\nfcCO///tzuKHP/whr3vd77Bu3cOzvSvNjI6OzvYubHO7774nf/EXV/GsZz1rtndlmxn4O2jPTeXS\nnYXYMfWnXR4CXl9VXxhYfhkwp6p+ewPrnMSO/5NKkqRt6Y1VdcXGBnfomY+qejTJ7cAC4AsASdK/\n/9hGVlsKvBH4DrDz/LNDkqQnb0/gl+l+lm7UDj3zAZDkd4HL6O5yWUZ398vvAAdX1Y9ncdckSdop\n7dAzHwBVdWX/TI+zgBHg74GFFg9JkmbHDj/zIUmSti879HM+JEnS9sfyIUmSmtrhr/nQzqW/vudU\nuifYzu0XrwJuAS7zWh9Jmn3OfGiH0X9b8beAdwOTwM39a7Jfdk+Sw2dvDyVtSpL9k1w62/uhbc8L\nTrXDSPK3dN9a/I4a+h+7f77L/wIOrSq/10faDiV5IbC8qnad7X3RtuVpF+1IXgicMlw8oPtCwSRL\ngDva75YkgCS/tZnI85rsiGad5UM7klXAkcA9Gxk/kv//SwYltfM5oIBsIuN0/E7A8qEdyZ8B/zvJ\nKHAj/140Rugeqf+fgffO0r5Jgh8C76yqz29oMMmLgNvb7pJmg+VDO4yquiDJBN0j9N8JTJ03fpzu\nL7RTqurK2do/SdwOjAIbLB9sflZEOwgvONUOqf9G4/36txNV9ehs7o8kSPISYK+qum4j43sBh1fV\nTW33TK1ZPiRJUlM+50OSJDVl+ZAkSU1ZPiRJUlOWD0mS1JTlQ5IkNWX5kLTdSbJ+Cx7FLennlOVD\nUnNJRpKcl+TbSR5O8t0kX0jy8tneN0nbnk84ldRUkl8CbgHuB/4LcBfwFOAVwPnAIbO3d5JacOZD\nUmsX0T3y/oiq+lxV/VNVraiqJcDRG1ohyYeT3JtkbT9bclaSXQfGD03yV0nWJJlM8ndJ5vdjB/Sz\nKvcn+bckdyZ5RZMjlbRBznxIaibJ04GFwB9W1cPD41W1ZiOrrgHeRPfFZC8APtEv+7N+/HJgOfB7\nwHrgRcDUI/UvpPu77teBh+hmVv5tBg5H0layfEhq6UC6Lw67dzorVdWfDLxdmeQjwIn8e/k4ADin\nqv6xf//tgfz+wFVVdXf//jvT3WlJM8vyIamlrfrG0iQnAqcD/wn4Rbq/uyYHIh8FLknyJuAG4P9W\n1T/3Yx8DLkqysB+7uqru3Mr9lzQDvOZDUkv/SPe16Qdv6QpJjgE+A3wReDXdKZX/Duw+lamq/0Z3\nOuWLwMsv5DkKAAABOElEQVSBbyZ5TT92CfBc4FPArwF/l+S0mTgYSVvHb7WV1FSSa+lKwEFV9dOh\nsTlVNZlkPfDaqvpCkvcAv19VvzKQuxh4XVXtu5FtXAE8tapeu4GxPwFeVVUvmsHDkjQNznxIau00\nYFdgWZLXJTkwycFJ3k13C+6wfwQOSHJikuf1uZ+ViiR79s8MeWl/Z8uLgSOAu/vxJUmOT/LL/R0w\nL5sakzQ7vOZDUlNVdV9fAv4r3QWjzwJ+DHwDeM9UbCD/l0mWAOcBewDXAGcBi/vI48AzgE8CI8AE\ncPXA+K50zw95Dt0dMl8a2I6kWeBpF0mS1JSnXSRJUlOWD0mS1JTlQ5IkNWX5kCRJTVk+JElSU5YP\nSZLUlOVDkiQ1ZfmQJElNWT4kSVJTlg9JktSU5UOSJDVl+ZAkSU39P5yQynuUeCcuAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ff2cf5f0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#reading the meta data and taking a look, we can see that there are a bunch of pre-prepped columns they can not disclose due \n",
    "#to confidentiality as well as time since the first transaction, which is likely useless, and amount. Amount is the only column\n",
    "#not normalized around 0. so lets normalize it and then look at the count \n",
    "data['Amount'] = StandardScaler().fit_transform(data['Amount'].reshape(-1, 1))\n",
    "data = data.drop('Time', axis=1)\n",
    "data['Class'].groupby(df['Class']).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#as we can see, there is a huge class imbalance so we are going to need to redefine the training set other wise our model will\n",
    "#just ignore the existance of fraud and get over a 99% success rate. This will not identify fraud at all\n",
    "X = data.drop('Class',axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of fraud transactions: 492\n",
      "Percentage of normal transactions:  0.5\n",
      "Percentage of fraud transactions:  0.5\n",
      "Total number of transactions in resampled data:  984\n"
     ]
    }
   ],
   "source": [
    "# Number of fraud\n",
    "number_of_fraud = len(data[data.Class == 1])\n",
    "fraud_indices = np.array(data[data.Class == 1].index)\n",
    "normal_indices = data[data.Class == 0].index\n",
    "\n",
    "# lets randomly select non fraud columns with the same length\n",
    "random_normal_indices = np.random.choice(normal_indices, number_of_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# adding them together\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "new_data = data.iloc[under_sample_indices,:]\n",
    "\n",
    "X_undersample = new_data.iloc[:, new_data.columns != 'Class']\n",
    "y_undersample = new_data.iloc[:, new_data.columns == 'Class']\n",
    "\n",
    "# lets check to make sure we did this right\n",
    "print('total number of fraud transactions:', number_of_fraud)\n",
    "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
    "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
    "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train test split for both the whole set and the sample\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 0)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_undersample,y_undersample,test_size = 0.25, random_state = 0)\n",
    "y_train2 = y_train2['Class'].values\n",
    "y_test2 = y_test2['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C= 1000\n",
      "Actual on top and predicted on left\n",
      "Class       0    1\n",
      "row_0             \n",
      "0      213201  144\n",
      "1          32  228\n",
      "accurately predicted non-fraud percentage\n",
      "0.99984992942\n",
      "accurately predicted fraud percentage\n",
      "0.612903225806\n",
      "Percentage accuracy\n",
      "0.99917604925\n",
      "0.99917604925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets test regression on the whole set\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "# Fit the model.\n",
    "fit = lr.fit(X_train, y_train)\n",
    "\n",
    "pred_y_sklearn = lr.predict(X_train)\n",
    "crosstab = pd.crosstab(pred_y_sklearn, y_train)\n",
    "print('C=', c)\n",
    "print('Actual on top and predicted on left')\n",
    "print(crosstab)\n",
    "print('accurately predicted non-fraud percentage')\n",
    "print(crosstab.iloc[0,0]/(crosstab.iloc[0,0]+crosstab.iloc[1,0]))\n",
    "print('accurately predicted fraud percentage')\n",
    "print(crosstab.iloc[1,1]/(crosstab.iloc[1,1]+crosstab.iloc[0,1]))\n",
    "print('Percentage accuracy')\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_train, y_train))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small train prediction\n",
      "Actual on top and predicted on left\n",
      "col_0    0    1\n",
      "row_0          \n",
      "0      358   25\n",
      "1        7  348\n",
      "accurately predicted non-fraud percentage\n",
      "0.980821917808\n",
      "accurately predicted fraud percentage\n",
      "0.932975871314\n",
      "Percentage accuracy\n",
      "0.956639566396\n",
      "0.956639566396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#as expected almost 100% accuracy with a terrible fraud percentage. lets use the small set\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "fit = lr.fit(X_train2, y_train2)\n",
    "pred_y_sklearn = lr.predict(X_train2)\n",
    "crosstab = pd.crosstab(pred_y_sklearn, y_train2)\n",
    "print('small train prediction')\n",
    "print('Actual on top and predicted on left')\n",
    "print(crosstab)\n",
    "print('accurately predicted non-fraud percentage')\n",
    "print(crosstab.iloc[0,0]/(crosstab.iloc[0,0]+crosstab.iloc[1,0]))\n",
    "print('accurately predicted fraud percentage')\n",
    "print(crosstab.iloc[1,1]/(crosstab.iloc[1,1]+crosstab.iloc[0,1]))\n",
    "print('Percentage accuracy')\n",
    "print(lr.score(X_train2, y_train2))\n",
    "print(lr.score(X_train2, y_train2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is way better but lets test to see what the best parameters for predicting fraud in this regression are\n",
    "outcome=pd.DataFrame()\n",
    "scores_test = []\n",
    "scores_train = []\n",
    "C = [.01,.05,.1,.5,1,5,10,50,100,500,1000]\n",
    "outcome['C'] = C\n",
    "for c in C:\n",
    "    lr = LogisticRegression(C=c, penalty='l1')\n",
    "    fit = lr.fit(X_train2, y_train2)\n",
    "    pred_y_sklearn = lr.predict(X_train2)\n",
    "    crosstab = pd.crosstab(pred_y_sklearn, y_train2)\n",
    "    scores_train.append(crosstab.iloc[1,1]/(crosstab.iloc[1,1]+crosstab.iloc[0,1]))\n",
    "    pred_y_sklearn = lr.predict(X_test2)\n",
    "    crosstab = pd.crosstab(pred_y_sklearn, y_test2)\n",
    "    scores_test.append(crosstab.iloc[1,1]/(crosstab.iloc[1,1]+crosstab.iloc[0,1])) \n",
    "outcome['train_scores'] = scores_train\n",
    "outcome['test_scores'] = scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1\n",
      "          C  train_scores  test_scores\n",
      "0      0.01      0.922252     0.941176\n",
      "1      0.05      0.890080     0.915966\n",
      "2      0.10      0.906166     0.915966\n",
      "3      0.50      0.924933     0.924370\n",
      "4      1.00      0.932976     0.932773\n",
      "5      5.00      0.930295     0.941176\n",
      "6     10.00      0.930295     0.941176\n",
      "7     50.00      0.932976     0.949580\n",
      "8    100.00      0.932976     0.941176\n",
      "9    500.00      0.932976     0.949580\n",
      "10  1000.00      0.932976     0.949580\n",
      "l2\n",
      "          C  train_scores  test_scores\n",
      "0      0.01      0.924933     0.932773\n",
      "1      0.05      0.919571     0.932773\n",
      "2      0.10      0.924933     0.941176\n",
      "3      0.50      0.932976     0.932773\n",
      "4      1.00      0.932976     0.932773\n",
      "5      5.00      0.930295     0.941176\n",
      "6     10.00      0.930295     0.941176\n",
      "7     50.00      0.932976     0.941176\n",
      "8    100.00      0.932976     0.949580\n",
      "9    500.00      0.941019     0.949580\n",
      "10  1000.00      0.941019     0.949580\n"
     ]
    }
   ],
   "source": [
    "outcome2=pd.DataFrame()\n",
    "scores_test = []\n",
    "scores_train = []\n",
    "outcome2['C'] = C\n",
    "for c in C:\n",
    "    lr = LogisticRegression(C=c)\n",
    "    fit = lr.fit(X_train2, y_train2)\n",
    "    pred_y_sklearn = lr.predict(X_train2)\n",
    "    crosstab = pd.crosstab(pred_y_sklearn, y_train2)\n",
    "    scores_train.append(crosstab.iloc[1,1]/(crosstab.iloc[1,1]+crosstab.iloc[0,1]))\n",
    "    pred_y_sklearn = lr.predict(X_test2)\n",
    "    crosstab = pd.crosstab(pred_y_sklearn, y_test2)\n",
    "    scores_test.append(crosstab.iloc[1,1]/(crosstab.iloc[1,1]+crosstab.iloc[0,1])) \n",
    "outcome2['train_scores'] = scores_train\n",
    "outcome2['test_scores'] = scores_test\n",
    "print('l1')\n",
    "print(outcome)\n",
    "print('l2')\n",
    "print(outcome2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set\n",
      "Actual on top and predicted on left\n",
      "Class       0    1\n",
      "row_0             \n",
      "0      201229   26\n",
      "1       12004  346\n",
      "accurately predicted non-fraud percentage\n",
      "0.943704773651\n",
      "accurately predicted fraud percentage\n",
      "0.930107526882\n",
      "Percentage accuracy\n",
      "0.943681093607\n",
      "0.943681093607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now that we have optimal parameters (either l1,C=50 or l2,C=10) lets test this on the total set to see if it still predicts well\n",
    "#when applied to a much bigger data set\n",
    "lr = LogisticRegression(C=50, penalty='l1')\n",
    "# Fit the model.\n",
    "fit = lr.fit(X_train2, y_train2)\n",
    "\n",
    "pred_y_sklearn = lr.predict(X_train)\n",
    "crosstab = pd.crosstab(pred_y_sklearn, y_train)\n",
    "print('train set')\n",
    "print('Actual on top and predicted on left')\n",
    "print(crosstab)\n",
    "print('accurately predicted non-fraud percentage')\n",
    "print(crosstab.iloc[0,0]/(crosstab.iloc[0,0]+crosstab.iloc[1,0]))\n",
    "print('accurately predicted fraud percentage')\n",
    "print(crosstab.iloc[1,1]/(crosstab.iloc[1,1]+crosstab.iloc[0,1]))\n",
    "print('Percentage accuracy')\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_train, y_train))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set\n",
      "Actual on top and predicted on left\n",
      "Class      0    1\n",
      "row_0            \n",
      "0      67090    5\n",
      "1       3992  115\n",
      "accurately predicted non-fraud percentage\n",
      "0.943839509299\n",
      "accurately predicted fraud percentage\n",
      "0.958333333333\n",
      "Percentage accuracy\n",
      "0.943681093607\n",
      "0.943681093607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y_sklearn = lr.predict(X_test)\n",
    "crosstab = pd.crosstab(pred_y_sklearn, y_test)\n",
    "print('test set')\n",
    "print('Actual on top and predicted on left')\n",
    "print(crosstab)\n",
    "print('accurately predicted non-fraud percentage')\n",
    "print(crosstab.iloc[0,0]/(crosstab.iloc[0,0]+crosstab.iloc[1,0]))\n",
    "print('accurately predicted fraud percentage')\n",
    "print(crosstab.iloc[1,1]/(crosstab.iloc[1,1]+crosstab.iloc[0,1]))\n",
    "print('Percentage accuracy')\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_train, y_train))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total set\n",
      "Actual on top and predicted on left\n",
      "Class       0    1\n",
      "row_0             \n",
      "0      268319   31\n",
      "1       15996  461\n",
      "accurately predicted non-fraud percentage\n",
      "0.943738459103\n",
      "accurately predicted fraud percentage\n",
      "0.936991869919\n",
      "Percentage accuracy\n",
      "0.943681093607\n",
      "0.943681093607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y_sklearn = lr.predict(X)\n",
    "crosstab = pd.crosstab(pred_y_sklearn, y)\n",
    "print('total set')\n",
    "print('Actual on top and predicted on left')\n",
    "print(crosstab)\n",
    "print('accurately predicted non-fraud percentage')\n",
    "print(crosstab.iloc[0,0]/(crosstab.iloc[0,0]+crosstab.iloc[1,0]))\n",
    "print('accurately predicted fraud percentage')\n",
    "print(crosstab.iloc[1,1]/(crosstab.iloc[1,1]+crosstab.iloc[0,1]))\n",
    "print('Percentage accuracy')\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_train, y_train))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we now have quite a few more false positives, however we are accurately predicting fraud over 93% of the time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
