{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge: Build your own NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "1. Data cleaning / processing / language parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the data for movie reviews and look at its contents\n",
    "# then we will clean the data\n",
    "from nltk.corpus import movie_reviews, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reviews 2000\n",
      "categories ['neg', 'pos']\n",
      "positive count 1000\n",
      "negative count 1000\n"
     ]
    }
   ],
   "source": [
    "print ('total reviews', len(movie_reviews.fileids()))\n",
    "print ('categories', movie_reviews.categories())\n",
    "print ('positive count', len(movie_reviews.fileids('pos')))\n",
    "print ('negative count', len(movie_reviews.fileids('neg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "\n",
      "(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg')\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(fileid), category))\n",
    " \n",
    "print(len(documents))\n",
    "print('')\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg'),\n",
       " (['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...], 'neg'),\n",
       " (['it', 'is', 'movies', 'like', 'these', 'that', 'make', ...], 'neg'),\n",
       " (['\"', 'quest', 'for', 'camelot', '\"', 'is', 'warner', ...], 'neg'),\n",
       " (['synopsis', ':', 'a', 'mentally', 'unstable', 'man', ...], 'neg')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle \n",
    "shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = [word.lower() for word in movie_reviews.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 39768 samples and 1583820 outcomes>\n",
      "\n",
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    " \n",
    "all_words_frequency = FreqDist(all_words)\n",
    " \n",
    "print (all_words_frequency)\n",
    "print('')\n",
    "# print 10 most frequently occurring words\n",
    "print (all_words_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'church', 'party', ',', 'drink']\n"
     ]
    }
   ],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "# create a new list of words by removing stopwords from all_words\n",
    "all_words_clean = [word for word in all_words if word not in stopwords_english]\n",
    " \n",
    "# print the first 10 words\n",
    "print(all_words_clean[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# create a new list of words by removing punctuation from all_words\n",
    "all_words_clean = [word for word in all_words_clean if word not in string.punctuation]\n",
    " \n",
    "# print the first 10 words\n",
    "print (all_words_clean[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 39586 samples and 710578 outcomes>\n"
     ]
    }
   ],
   "source": [
    "all_words_frequency = FreqDist(all_words_clean)\n",
    "print (all_words_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 9517), ('one', 5852), ('movie', 5771), ('like', 3690), ('even', 2565), ('good', 2411), ('time', 2411), ('story', 2169), ('would', 2109), ('much', 2049)]\n",
      "\n",
      "[('asking', 64), ('niro', 64), ('path', 64), ('aware', 64), ('remain', 64), ('rain', 64), ('exact', 64), ('international', 64), ('moved', 64), ('anna', 64)]\n",
      "\n",
      "['film', 'one', 'movie', 'like', 'even', 'good', 'time', 'story', 'would', 'much']\n"
     ]
    }
   ],
   "source": [
    "most_common_words = all_words_frequency.most_common(2000)\n",
    "print (most_common_words[:10])\n",
    "print('')\n",
    "print (most_common_words[1990:])\n",
    "print('')\n",
    "word_features = [item[0] for item in most_common_words]\n",
    "print (word_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [nlp(str(parag[0])) for parag in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_ = [parag[1] for parag in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [(nlp(str(parag[0])), parag[1]) for parag in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for parag in documents:\n",
    "    p = str(parag[0])\n",
    "    l.append(nlp(''.join(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [sent for sent in l for sub in sent.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2002"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = sentences[:len(sentences)-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "3. Use the features to fit supervised learning models for each feature set to predict the category outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences['sentences']\n",
    "    df['text_source'] = sentences['sentiment']\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Set up the bags.\n",
    "allwords = [bag_of_words(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dicillo', 'direct', 'superficial', 'tom'],\n",
       " ['little', 'actually', 'saint'],\n",
       " ['s', 'line', 'thin'],\n",
       " ['film', 'see', 'worth'],\n",
       " ['year', '14', 'lampoon', 'ago', 'national'],\n",
       " ['fiction', 'realm', 'science'],\n",
       " ['good', 'formula', 'feel'],\n",
       " ['reflect', 'bedazzle'],\n",
       " ['douglas', 'kirk', 'rare'],\n",
       " ['thing']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_word = set([sent for words in allwords for sent in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_df = pd.DataFrame({'sentences': sentences, 'sentiment':pos_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2444"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_common_word = [word for word in common_word if word not in stopwords_english]\n",
    "clean_common_word = [word for word in clean_common_word if word not in string.punctuation]\n",
    "len(clean_common_word)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attitude</th>\n",
       "      <th>travel</th>\n",
       "      <th>inch</th>\n",
       "      <th>miracle</th>\n",
       "      <th>coincide</th>\n",
       "      <th>rosenthal</th>\n",
       "      <th>mess</th>\n",
       "      <th>nicolas</th>\n",
       "      <th>tod</th>\n",
       "      <th>describe</th>\n",
       "      <th>...</th>\n",
       "      <th>trekkie</th>\n",
       "      <th>triumphant</th>\n",
       "      <th>chew</th>\n",
       "      <th>ghost</th>\n",
       "      <th>beauty</th>\n",
       "      <th>highly</th>\n",
       "      <th>lange</th>\n",
       "      <th>kar</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>([, ', tom, ', ,, ', dicillo, ', ,, ', directs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>([, ', the, ', ,, ', saint, ', ,, ', was, ', ,...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>([, ', there, ', ,, \", ', \", ,, ', s, ', ,, ',...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>([, ', this, ', ,, ', film, ', ,, ', is, ', ,,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>([, ', 14, ', ,, ', years, ', ,, ', ago, ', ,,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2425 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  attitude travel inch miracle coincide rosenthal mess nicolas tod describe  \\\n",
       "0        0      0    0       0        0         0    0       0   0        0   \n",
       "1        0      0    0       0        0         0    0       0   0        0   \n",
       "2        0      0    0       0        0         0    0       0   0        0   \n",
       "3        0      0    0       0        0         0    0       0   0        0   \n",
       "4        0      0    0       0        0         0    0       0   0        0   \n",
       "\n",
       "      ...     trekkie triumphant chew ghost beauty highly lange kar  \\\n",
       "0     ...           0          0    0     0      0      0     0   0   \n",
       "1     ...           0          0    0     0      0      0     0   0   \n",
       "2     ...           0          0    0     0      0      0     0   0   \n",
       "3     ...           0          0    0     0      0      0     0   0   \n",
       "4     ...           0          0    0     0      0      0     0   0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  ([, ', tom, ', ,, ', dicillo, ', ,, ', directs...         neg  \n",
       "1  ([, ', the, ', ,, ', saint, ', ,, ', was, ', ,...         pos  \n",
       "2  ([, ', there, ', ,, \", ', \", ,, ', s, ', ,, ',...         pos  \n",
       "3  ([, ', this, ', ,, ', film, ', ,, ', is, ', ,,...         neg  \n",
       "4  ([, ', 14, ', ,, ', years, ', ,, ', ago, ', ,,...         neg  \n",
       "\n",
       "[5 rows x 2425 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = bow_features(sentences_df, clean_common_word)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9575\n",
      "\n",
      "Test set score: 0.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = word_counts['text_source']\n",
    "X = np.array(word_counts.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 2423) (1200,)\n",
      "Training set score: 0.9108333333333334\n",
      "\n",
      "Test set score: 0.5125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.6916666666666667\n",
      "\n",
      "Test set score: 0.4875\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5044985104865345\n"
     ]
    }
   ],
   "source": [
    "# random forest had the highest test score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(rfc.fit(X, Y), X, Y).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_df = sentences_df.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test = train_test_split(sentences_df, test_size=0.4, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 848\n"
     ]
    }
   ],
   "source": [
    "#Applying the vectorizer\n",
    "tfidf = vectorizer.fit_transform(sentences_df['sentences'])\n",
    "print(\"Number of features: %d\" % tfidf.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#splitting into training and test sets\n",
    "X_train2, X_test2 = train_test_split(tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_csr = X_train2.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_csr[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: sentences    ['in', '\"', 'magic', 'town', '\"', ',', 'jimmy'...\n",
      "sentiment                                                  pos\n",
      "Name: 891, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train.loc[891])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf_idf vector: {'review': 0.3529432108644767, 'entertainment': 0.5261192114969359, 'fully': 0.5470967305689717, 'loaded': 0.5470967305689717}\n"
     ]
    }
   ],
   "source": [
    "print('Tf_idf vector:', tfidf_bypara[891])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 50.379780636707395\n",
      "Component 0:\n",
      "(['any', 'movie', 'about', 'the', 'underground', ...], pos)            0.986503\n",
      "(['as', 'with', 'any', 'gen', '-', 'x', 'mtv', 'movie', ...], pos)     0.986503\n",
      "(['ugh', '.', 'that', 'about', 'sums', 'this', 'movie', ...], neg)     0.986503\n",
      "(['this', 'is', 'the', 'movie', 'not', 'the', 'perfume', ...], pos)    0.986503\n",
      "(['this', 'is', 'the', 'movie', 'that', 'could', ...], neg)            0.986503\n",
      "(['here', 'is', 'a', 'movie', 'that', 'sadly', ...], neg)              0.986503\n",
      "(['\"', 'gordy', '\"', 'is', 'not', 'a', 'movie', ',', ...], pos)        0.986503\n",
      "(['i', 'looked', 'at', 'the', '\"', 'internet', 'movie', ...], neg)     0.986503\n",
      "(['a', 'movie', 'about', 'divorce', 'and', 'custody', ...], neg)       0.986503\n",
      "(['a', 'movie', 'that', \"'\", 's', 'been', 'as', ...], pos)             0.986503\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "(['every', 'once', 'in', 'a', 'while', ',', 'a', 'film', ...], pos)    0.988513\n",
      "(['everybody', 'in', 'this', 'film', \"'\", 's', ...], neg)              0.988513\n",
      "(['here', 'is', 'a', 'film', 'that', 'is', 'so', ...], pos)            0.988513\n",
      "(['this', 'film', 'is', 'extraordinarily', 'horrendous', ...], neg)    0.988513\n",
      "(['the', 'reunion', 'film', 'is', 'not', 'an', ...], pos)              0.988513\n",
      "(['for', 'a', 'film', 'touted', 'as', 'exploring', ...], neg)          0.988513\n",
      "(['alien', '3', 'is', 'the', 'only', 'alien', 'film', ...], pos)       0.988513\n",
      "(['on', 'the', 'basis', 'of', 'this', 'film', 'alone', ...], pos)      0.988513\n",
      "(['this', 'is', 'a', 'stagy', 'film', 'adapted', 'from', ...], pos)    0.984278\n",
      "(['the', 'film', '\"', 'magnolia', '\"', 'can', 'be', ...], pos)         0.962329\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "(['synopsis', ':', 'al', 'simmons', ',', 'top', '-', ...], pos)        0.994833\n",
      "(['synopsis', ':', 'two', 'con', 'artists', 'find', ...], neg)         0.994833\n",
      "(['synopsis', ':', 'committed', 'to', 'an', 'asylum', ...], pos)       0.994833\n",
      "(['synopsis', ':', 'cro', '-', 'magnon', 'ayla', ...], neg)            0.994833\n",
      "(['synopsis', ':', 'bobby', 'garfield', '(', 'yelchin', ...], pos)     0.994833\n",
      "(['synopsis', ':', 'blond', 'criminal', 'psychologist', ...], neg)     0.994833\n",
      "(['synopsis', ':', 'in', 'this', 'cultural', ...], pos)                0.994833\n",
      "(['synopsis', ':', 'in', '\"', 'sooner', 'than', 'you', ...], neg)      0.994833\n",
      "(['synopsis', ':', 'rumored', 'to', 'be', 'about', 'a', ...], pos)     0.994833\n",
      "(['synopsis', ':', 'an', 'attractive', 'mute', 'makeup', ...], pos)    0.994833\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], neg)    0.940189\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], neg)    0.940189\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], neg)    0.940189\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], neg)    0.940189\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], pos)    0.940189\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], pos)    0.940189\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], neg)    0.940189\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], neg)    0.940189\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], pos)    0.940189\n",
      "(['susan', 'granger', \"'\", 's', 'review', 'of', '\"', ...], pos)    0.940189\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "(['if', 'i', 'were', 'to', 'plot', 'a', 'graph', 'of', ...], neg)      0.937178\n",
      "(['plot', ':', 'upon', 'the', 'realization', 'that', ...], neg)        0.937178\n",
      "(['*', '*', '*', '*', '*', '*', 'minor', 'plot', ...], pos)            0.937178\n",
      "(['plot', 'outline', '-', 'wendy', '(', 'samantha', ...], pos)         0.937178\n",
      "(['plot', ':', 'a', 'separated', ',', 'glamorous', ',', ...], neg)     0.937178\n",
      "(['plot', ':', 'something', 'about', 'a', 'bunch', 'of', ...], neg)    0.925664\n",
      "(['plot', ':', 'a', 'group', 'of', 'asbestos', ...], pos)              0.922131\n",
      "(['plot', ':', 'a', 'human', 'space', 'astronaut', ...], pos)          0.917080\n",
      "(['plot', ':', 'lara', 'croft', 'is', 'british', ',', ...], neg)       0.903094\n",
      "(['weighed', 'down', 'by', 'tired', 'plot', 'lines', ...], neg)        0.890546\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train2)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Pick one of the models and try to increase accuracy by at least 5 percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 2702\n"
     ]
    }
   ],
   "source": [
    "# BOW vastly overfit and performer worse. lets try to improve idf\n",
    "vectorizer = TfidfVectorizer(max_df=0.35, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=1, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "tfidf = vectorizer.fit_transform(sentences_df['sentences'])\n",
    "print(\"Number of features: %d\" % tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train2, X_test2 = train_test_split(tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_csr = X_train2.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_csr[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 33.93463522067122\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train2)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 432\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=3, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "tfidf = vectorizer.fit_transform(sentences_df['sentences'])\n",
    "print(\"Number of features: %d\" % tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train2, X_test2 = train_test_split(tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_csr = X_train2.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_csr[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 65.74068325145156\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 1379 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train2)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
